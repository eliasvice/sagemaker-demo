{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf  # pylint: disable=g-bad-import-order\n",
    "\n",
    "# from official.utils.arg_parsers import parsers\n",
    "# from official.utils.logs import hooks_helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fields = [\n",
    " 'trafficsource_referralpath',\n",
    " 'geonetwork_city',\n",
    " 'geonetwork_country',\n",
    " 'geonetwork_region',\n",
    " 'geonetwork_metro',\n",
    " 'device_browser',\n",
    " 'trafficsource_campaign',\n",
    " 'trafficsource_source',\n",
    " 'trafficsource_medium',\n",
    " 'device_mobiledevicemodel',\n",
    " 'device_operatingsystemversion',\n",
    " 'hits_appinfo_appname',\n",
    " 'hits_appinfo_appversion',\n",
    " 'hits_appinfo_screenname',\n",
    " 'hits_page_pagetitle',\n",
    " 'hits_page_pagepath'\n",
    "]\n",
    "\n",
    "_metrics = [\n",
    "    'pageviews',\n",
    "    'screenviews'\n",
    "]\n",
    "_label = ['article_consumption_level']\n",
    "\n",
    "_CSV_COLUMNS = _fields + _metrics + _label\n",
    "\n",
    "_fields_defaults = [['']] * len(_fields)\n",
    "_metrics_defaults = [[0]] * len(_metrics)\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS = _fields_defaults + _metrics_defaults + [[0]]\n",
    "\n",
    "_target_labels_idx = {\n",
    "    '50': 0,\n",
    "    '75': 1,\n",
    "    '100': 2\n",
    "}\n",
    "\n",
    "# TODO: Select small dataset for \n",
    "_NUM_EXAMPLES = {\n",
    "    'train': 32561,\n",
    "    'validation': 16281,\n",
    "}\n",
    "\n",
    "\n",
    "LOSS_PREFIX = {'wide': 'linear/', 'deep': 'dnn/'}\n",
    "\n",
    "def build_model_columns():\n",
    "    \"\"\"\n",
    "        Builds a set of wide and deep feature columns.\n",
    "    \"\"\"\n",
    "    # Continuous columns\n",
    "\n",
    "    pageviews = tf.feature_column.numeric_column('pageviews')\n",
    "    screenviews = tf.feature_column.numeric_column('screenviews')\n",
    "\n",
    "    geonetwork_country = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'geonetwork_country', hash_bucket_size=200)\n",
    "    geonetwork_region = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'geonetwork_region', hash_bucket_size=500)\n",
    "#     hits_hour = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "#         'hits_hour', hash_bucket_size=20\n",
    "#     )\n",
    "    trafficsource_referralpath = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'trafficsource_referralpath', hash_bucket_size=25000)\n",
    "    geonetwork_city = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'geonetwork_city', hash_bucket_size=5000)\n",
    "    geonetwork_metro = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'geonetwork_metro', hash_bucket_size=200)\n",
    "    device_browser = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'device_browser', hash_bucket_size=200)\n",
    "    trafficsource_campaign = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'trafficsource_campaign', hash_bucket_size=10000)\n",
    "    trafficsource_source = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'trafficsource_source', hash_bucket_size=200)\n",
    "    trafficsource_medium = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'trafficsource_medium', hash_bucket_size=300)\n",
    "    device_mobiledevicemodel = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'device_mobiledevicemodel', hash_bucket_size=500)\n",
    "    device_operatingsystemversion = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'device_operatingsystemversion', hash_bucket_size=100)\n",
    "    hits_appinfo_appname = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'hits_appinfo_appname', hash_bucket_size=500)\n",
    "    hits_appinfo_appversion = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'hits_appinfo_appversion', hash_bucket_size=25)\n",
    "    hits_appinfo_screenname = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'hits_appinfo_screenname', hash_bucket_size=12000)\n",
    "    hits_page_pagetitle = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'hits_page_pagetitle', hash_bucket_size=18000)\n",
    "    hits_page_pagepath = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        'hits_page_pagepath', hash_bucket_size=15000)\n",
    "\n",
    "    # Wide columns and deep columns.\n",
    "    base_columns = [\n",
    "        pageviews, screenviews, geonetwork_region, geonetwork_city, geonetwork_metro, geonetwork_country, \n",
    "        device_browser, trafficsource_campaign, trafficsource_source, trafficsource_medium, device_mobiledevicemodel,\n",
    "        device_operatingsystemversion, hits_appinfo_appname, hits_appinfo_appversion, hits_appinfo_screenname,\n",
    "        hits_page_pagetitle, hits_page_pagepath, trafficsource_referralpath\n",
    "    ]\n",
    "\n",
    "    crossed_columns = [\n",
    "      tf.feature_column.crossed_column(\n",
    "          ['hits_appinfo_appname', 'hits_appinfo_appversion'], hash_bucket_size=5000),\n",
    "      tf.feature_column.crossed_column(\n",
    "          ['geonetwork_city', 'hits_page_pagepath'], hash_bucket_size=25000),\n",
    "      tf.feature_column.crossed_column(\n",
    "          ['geonetwork_city', 'geonetwork_metro'], hash_bucket_size=25000)\n",
    "    ]\n",
    "\n",
    "    wide_columns = base_columns + crossed_columns\n",
    "\n",
    "    deep_columns = [\n",
    "        pageviews,\n",
    "        screenviews,\n",
    "        tf.feature_column.embedding_column(geonetwork_metro, dimension=8),\n",
    "        tf.feature_column.embedding_column(geonetwork_city, dimension=8),\n",
    "        tf.feature_column.embedding_column(geonetwork_country, dimension=8),\n",
    "        tf.feature_column.embedding_column(device_browser, dimension=8),\n",
    "        tf.feature_column.embedding_column(trafficsource_source, dimension=8),\n",
    "        tf.feature_column.embedding_column(trafficsource_campaign, dimension=8),\n",
    "        tf.feature_column.embedding_column(device_operatingsystemversion, dimension=8),\n",
    "        tf.feature_column.embedding_column(device_mobiledevicemodel, dimension=8),\n",
    "        tf.feature_column.embedding_column(hits_appinfo_appname, dimension=8),\n",
    "        tf.feature_column.embedding_column(hits_appinfo_appversion, dimension=8),\n",
    "        tf.feature_column.embedding_column(hits_page_pagetitle, dimension=8),\n",
    "        tf.feature_column.embedding_column(hits_page_pagepath, dimension=8),\n",
    "        tf.feature_column.embedding_column(trafficsource_referralpath, dimension=8),\n",
    "        tf.feature_column.embedding_column(trafficsource_medium, dimension=8)\n",
    "    ]\n",
    "\n",
    "    return wide_columns, deep_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "  wide_columns, deep_columns = build_model_columns()\n",
    "  hidden_units = [20, 15, 10, 5]\n",
    "\n",
    "  # Create a tf.estimator.RunConfig to ensure the model is run on CPU, which\n",
    "  # trains faster than GPU for this model.\n",
    "  run_config = tf.estimator.RunConfig().replace(\n",
    "      session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "  if model_type == 'wide':\n",
    "    print('wide')\n",
    "    return tf.estimator.LinearClassifier(\n",
    "        model_dir=model_dir,\n",
    "        n_classes=3,\n",
    "        feature_columns=wide_columns,\n",
    "        config=run_config)\n",
    "  elif model_type == 'deep':\n",
    "    print('deep')\n",
    "    return tf.estimator.DNNClassifier(\n",
    "        model_dir=model_dir,\n",
    "        n_classes=3,\n",
    "        feature_columns=deep_columns,\n",
    "        hidden_units=hidden_units,\n",
    "        config=run_config)\n",
    "  else:\n",
    "    print (\"wide + deep\")\n",
    "    return tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        n_classes=3,\n",
    "        linear_optimizer=tf.train.FtrlOptimizer(\n",
    "            learning_rate=0.01,\n",
    "            l1_regularization_strength=0.0001,\n",
    "            l2_regularization_strength=0.01),\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "    assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have run data_download.py and '\n",
    "      'set the --data_dir argument to the correct path.' % data_file)\n",
    "\n",
    "    def parse_csv(value):\n",
    "        print('Parsing', data_file)\n",
    "        columns = tf.decode_csv(value, field_delim='|', record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop('article_consumption_level')\n",
    "        return features, labels\n",
    "\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])\n",
    "\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=4)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wide + deep\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../model_dir/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11e574358>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model_dir = \"../model_dir/\"\n",
    "model_type = 'wide+deep'\n",
    "# model_type = 'wide'\n",
    "# model_type = 'deep'\n",
    "data_dir = '../data/'\n",
    "\n",
    "train_epochs = 20\n",
    "epochs_between_evals = 10\n",
    "\n",
    "# Clean up the model directory if present\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "model = build_estimator(model_dir, model_type)\n",
    "\n",
    "train_file = os.path.join(data_dir, 'scroll_traffic.train.small')\n",
    "validate_file = os.path.join(data_dir, 'scroll_traffic.valid.small')\n",
    "test_file = os.path.join(data_dir, 'scroll_traffic.test.small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 'head/truediv', 'loss': 'head/weighted_loss/Sum'}\n",
      "Parsing ../data/scroll_traffic.train.small\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ../model_dir/model.ckpt.\n",
      "INFO:tensorflow:loss = 82.11897, step = 1\n",
      "INFO:tensorflow:global_step/sec: 66.6932\n",
      "INFO:tensorflow:loss = 48.97396, step = 101 (1.500 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.347\n",
      "INFO:tensorflow:loss = 39.52893, step = 201 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.51\n",
      "INFO:tensorflow:loss = 36.83058, step = 301 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.923\n",
      "INFO:tensorflow:loss = 41.057983, step = 401 (0.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.125\n",
      "INFO:tensorflow:loss = 42.43494, step = 501 (0.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 129\n",
      "INFO:tensorflow:loss = 42.32032, step = 601 (0.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.464\n",
      "INFO:tensorflow:loss = 31.224398, step = 701 (0.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.011\n",
      "INFO:tensorflow:loss = 47.030586, step = 801 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.793\n",
      "INFO:tensorflow:loss = 37.13787, step = 901 (0.650 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.145\n",
      "INFO:tensorflow:loss = 34.79256, step = 1001 (0.681 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.583\n",
      "INFO:tensorflow:loss = 35.50201, step = 1101 (0.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.038\n",
      "INFO:tensorflow:loss = 36.84986, step = 1201 (0.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.912\n",
      "INFO:tensorflow:loss = 46.926483, step = 1301 (0.747 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.965\n",
      "INFO:tensorflow:loss = 36.924057, step = 1401 (0.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.035\n",
      "INFO:tensorflow:loss = 43.79193, step = 1501 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.713\n",
      "INFO:tensorflow:loss = 29.392628, step = 1601 (0.809 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.878\n",
      "INFO:tensorflow:loss = 41.019978, step = 1701 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 123.523\n",
      "INFO:tensorflow:loss = 37.769543, step = 1801 (0.810 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.331\n",
      "INFO:tensorflow:loss = 41.69041, step = 1901 (0.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.209\n",
      "INFO:tensorflow:loss = 35.1631, step = 2001 (0.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.368\n",
      "INFO:tensorflow:loss = 45.26528, step = 2101 (0.778 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.584\n",
      "INFO:tensorflow:loss = 31.631296, step = 2201 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.7\n",
      "INFO:tensorflow:loss = 31.034416, step = 2301 (0.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.911\n",
      "INFO:tensorflow:loss = 37.178925, step = 2401 (0.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.623\n",
      "INFO:tensorflow:loss = 39.145016, step = 2501 (0.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.302\n",
      "INFO:tensorflow:loss = 44.081604, step = 2601 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.447\n",
      "INFO:tensorflow:loss = 34.920452, step = 2701 (0.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.474\n",
      "INFO:tensorflow:loss = 31.56109, step = 2801 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 145.756\n",
      "INFO:tensorflow:loss = 32.618385, step = 2901 (0.685 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.454\n",
      "INFO:tensorflow:loss = 25.461792, step = 3001 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.784\n",
      "INFO:tensorflow:loss = 31.414082, step = 3101 (0.748 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3125 into ../model_dir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 32.11508.\n",
      "Parsing ../data/scroll_traffic.valid.small\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-01-14:39:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../model_dir/model.ckpt-3125\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-01-14:39:26\n",
      "INFO:tensorflow:Saving dict for global step 3125: accuracy = 0.7881, average_loss = 0.62730604, global_step = 3125, loss = 39.9558\n",
      "Results at epoch 10\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.7881\n",
      "average_loss: 0.62730604\n",
      "global_step: 3125\n",
      "loss: 39.9558\n",
      "Parsing ../data/scroll_traffic.train.small\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../model_dir/model.ckpt-3125\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3126 into ../model_dir/model.ckpt.\n",
      "INFO:tensorflow:loss = 31.289999, step = 3126\n",
      "INFO:tensorflow:global_step/sec: 64.604\n",
      "INFO:tensorflow:loss = 31.266277, step = 3226 (1.549 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.582\n",
      "INFO:tensorflow:loss = 32.61633, step = 3326 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.412\n",
      "INFO:tensorflow:loss = 36.48304, step = 3426 (0.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 143.912\n",
      "INFO:tensorflow:loss = 32.19491, step = 3526 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.841\n",
      "INFO:tensorflow:loss = 31.285568, step = 3626 (0.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.555\n",
      "INFO:tensorflow:loss = 28.096275, step = 3726 (0.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.686\n",
      "INFO:tensorflow:loss = 36.50406, step = 3826 (0.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.773\n",
      "INFO:tensorflow:loss = 22.55938, step = 3926 (0.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.073\n",
      "INFO:tensorflow:loss = 29.503727, step = 4026 (0.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.495\n",
      "INFO:tensorflow:loss = 26.255564, step = 4126 (0.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.992\n",
      "INFO:tensorflow:loss = 42.924355, step = 4226 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.261\n",
      "INFO:tensorflow:loss = 46.222095, step = 4326 (0.636 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.604\n",
      "INFO:tensorflow:loss = 37.303932, step = 4426 (0.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 155.518\n",
      "INFO:tensorflow:loss = 26.267723, step = 4526 (0.643 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.935\n",
      "INFO:tensorflow:loss = 31.596191, step = 4626 (0.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.145\n",
      "INFO:tensorflow:loss = 35.059807, step = 4726 (0.693 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.441\n",
      "INFO:tensorflow:loss = 39.76361, step = 4826 (0.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 151.03\n",
      "INFO:tensorflow:loss = 33.03254, step = 4926 (0.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 144.384\n",
      "INFO:tensorflow:loss = 34.496452, step = 5026 (0.692 sec)\n",
      "INFO:tensorflow:global_step/sec: 166.754\n",
      "INFO:tensorflow:loss = 30.837116, step = 5126 (0.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.947\n",
      "INFO:tensorflow:loss = 32.66607, step = 5226 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.702\n",
      "INFO:tensorflow:loss = 41.47293, step = 5326 (0.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.407\n",
      "INFO:tensorflow:loss = 40.706474, step = 5426 (0.756 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.765\n",
      "INFO:tensorflow:loss = 41.625534, step = 5526 (0.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.78\n",
      "INFO:tensorflow:loss = 26.851856, step = 5626 (0.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 167.441\n",
      "INFO:tensorflow:loss = 30.6996, step = 5726 (0.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.44\n",
      "INFO:tensorflow:loss = 42.83918, step = 5826 (0.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.052\n",
      "INFO:tensorflow:loss = 33.488766, step = 5926 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.298\n",
      "INFO:tensorflow:loss = 31.773626, step = 6026 (0.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.969\n",
      "INFO:tensorflow:loss = 36.97407, step = 6126 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.191\n",
      "INFO:tensorflow:loss = 27.507687, step = 6226 (0.574 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6250 into ../model_dir/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 34.811333.\n",
      "Parsing ../data/scroll_traffic.valid.small\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-01-14:40:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../model_dir/model.ckpt-6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-01-14:40:03\n",
      "INFO:tensorflow:Saving dict for global step 6250: accuracy = 0.7879, average_loss = 0.6271991, global_step = 6250, loss = 39.94899\n",
      "Results at epoch 20\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.7879\n",
      "average_loss: 0.6271991\n",
      "global_step: 6250\n",
      "loss: 39.94899\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and evaluate the model every `flags.epochs_between_evals` epochs.\n",
    "def train_input_fn():\n",
    "    return input_fn(\n",
    "        train_file, epochs_between_evals, True, batch_size)\n",
    "\n",
    "def eval_input_fn():\n",
    "    return input_fn(validate_file, 1, False, batch_size)\n",
    "\n",
    "def test_input_fn():\n",
    "    return input_fn(test_file, 1, False, batch_size)\n",
    "\n",
    "loss_prefix = LOSS_PREFIX.get(model_type, '')\n",
    "# train_hooks = hooks_helper.get_train_hooks(\n",
    "#   hooks, batch_size=batch_size,\n",
    "tensors_to_log={'average_loss': loss_prefix + 'head/truediv',\n",
    "                  'loss': loss_prefix + 'head/weighted_loss/Sum'}\n",
    "print(tensors_to_log)\n",
    "\n",
    "# Train and evaluate the model every `flags.epochs_between_evals` epochs.\n",
    "for n in range(train_epochs // epochs_between_evals):\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    results = model.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "    # Display evaluation metrics\n",
    "    print('Results at epoch', (n + 1) * epochs_between_evals)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "        print('%s: %s' % (key, results[key]))\n",
    "\n",
    "#     if model_helpers.past_stop_threshold(\n",
    "#         stop_threshold, results['accuracy']):\n",
    "#         break\n",
    "\n",
    "# pred_and_probs = list(model.predict(input_fn=test_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ../data/scroll.test.small\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../model_dir/model.ckpt-10932\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "preds = list(model.predict(input_fn=test_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35904847])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_classes = [x['class_ids'] for x in preds]\n",
    "pred_probs = [x['probabilities'] for x in preds]\n",
    "perc_not_2 = sum(np.array(pred_classes) != 2)\n",
    "perc_not_2/len(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8996"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definitely the class imbalance is causing the model to predict class 2 in most cases than the other classes\n",
    "# This can be mitigated with a more balanced training and testing set\n",
    "len(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32171628, 0.30503032, 0.37325343],\n",
       "       [0.14107876, 0.27234718, 0.5865741 ],\n",
       "       [0.46001828, 0.3357787 , 0.20420308],\n",
       "       [0.13506429, 0.18011568, 0.68482006]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_probs)[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8996, 19)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def get_pred(arr, cutoff=.7):\n",
    "    if arr[-1] < cutoff:\n",
    "        return np.argmax(arr[:-1])\n",
    "    else:\n",
    "        return np.argmax(arr)\n",
    "df = pd.read_table(\"../data/scroll.test.small\", sep='|', header=None, names=_CSV_COLUMNS)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8996"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_preds = [get_pred(x) for x in pred_probs]\n",
    "len(my_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.28      0.31      2000\n",
      "          1       0.34      0.27      0.30      1996\n",
      "          2       0.66      0.76      0.70      5000\n",
      "\n",
      "avg / total       0.52      0.54      0.53      8996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.array(df[_CSV_COLUMNS[-1]]), np.array(pred_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.26      0.56      0.35      2000\n",
      "          1       0.26      0.51      0.34      1996\n",
      "          2       0.92      0.14      0.25      5000\n",
      "\n",
      "avg / total       0.63      0.32      0.29      8996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.array(df[_CSV_COLUMNS[-1]]), np.array(my_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1112  859   29]\n",
      " [ 953 1013   30]\n",
      " [2263 2016  721]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(np.array(df[_CSV_COLUMNS[-1]]), np.array(my_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
